# Arena KV-Cache for Mistral-7B
**High-Performance Memory Manager for Transformer Key/Value Tensors Mistral-7B**

Arena-Allocated KV-Cache with **Slab Recycling** & **True Zero-Copy Extensions** â€“ a production-ready, low-fragmentation memory manager for LLM servers, written in Rust with seamless Python bindings.

---

## ğŸš€ Why Arena KV-Cache?

| Pain Point | Symptom in Production | Arena KV-Cache Solution |
|------------|----------------------|-------------------------|
| **Memory Fragmentation** | Contexts of widely varying lengths leave â€œholesâ€ in the CUDA heap â†’ OOM long before theoretical capacity | **Slab Recycling** â€“ pages automatically return to lock-free pools for reuse |
| **Copy Amplification** | Every generation step copies KV tensors into a larger buffer â†’ higher latency | **True Zero-Copy** â€“ only atomic metadata updates; data never moves |
| **GC Stalls** | Whole-tensor drops trigger synchronous device-side frees | **Bump Allocation** â€“ `offset += align(size)`; no per-tensor metadata, instant cleanup |

---

## âœ¨ Key Features

- **ğŸï¸ True Zero-Copy Extensions**  
  Sub-microsecond tensor growth using atomic pointer/length updates.

- **â™»ï¸ Lock-Free Slab Recycling**  
  Pages are recycled back into a global `SegQueue<Page>` when an arena drops.

- **ğŸ¯ KV-Optimized Page Sizing**  
  Configure `PAGE_BYTES = ceil(max_expected_tensor_bytes)` to eliminate internal waste.

- **ğŸ”¥ CUDA-First Design**  
  Tuned for Tesla T4, V100, A100, H100; falls back gracefully to CPU.

- **âš¡ Bump Allocation**  
  O(1)â€†allocation & free with zero per-tensor overhead.

- **ğŸ Python Integration**  
  `pyo3` bindings expose `ArenaKVCacheManager` and `SequenceArena` as native PyTorch
  tensors (`torch.Tensor`) on CUDA.

- **ğŸ“Š Production Metrics**  
  Built-in Prometheus/Opentelemetry counters: slab hit %, bytes recycled /s, alloc p95, etc.

---